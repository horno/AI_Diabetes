# -*- coding: utf-8 -*-
"""Diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ED-J5N48qUyqdkwun_wngAsdKtZKGsWN

Primer s'importen els mòduls de Python necessaris
"""

import sys
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import sklearn.datasets
print("Python version: {}".format(sys.version))

"""Descarreguem el dataset de la diabetes i l'assginem al nom diabetes."""

diabetes = sklearn.datasets.load_diabetes()

"""Separem les dades en conjunts d'entrenament i conjunts per testejar"""

diabetes_X = diabetes.data[:, np.newaxis, 2]
train_X = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]
n_samples = train_X.shape[0]
print n_samples

"""Separem les targets en entrenament i testos"""

train_Y = diabetes.target[:-20]
diabetes_Y_test = diabetes.target[-20:]

"""Mostrem gràficament les dades"""

plt.plot(train_X, train_Y, 'ro', label='Original data')
plt.legend()
plt.show()

"""Creació del graf d'operacions"""

class LinearRegressionModel:
  def __init__(self, learning_rate):
    self.X = tf.placeholder(tf.float32, name="X")
    self.Y = tf.placeholder(tf.float32, name="Y")

    # Set model weights
    self.W = tf.Variable(tf.truncated_normal([1]), name="weight")
    self.b = tf.Variable(tf.truncated_normal([1]), name="bias")

    self.Y_pred = tf.add(tf.multiply(self.X, self.W), self.b, name="hypothesis")

    # Mean squared error
    self.cost = tf.reduce_sum(tf.pow(self.Y_pred-self.Y, 2))/(2*n_samples)
    # Gradient descent
    self.optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.cost)

    

learning_rate =0.01
lr_model = LinearRegressionModel(learning_rate)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

"""Creació de la sessió de minimització de costos"""

training_epochs = 1500
display_step = 50

total_epochs = []
total_loss = []

# Start training
with tf.Session() as sess:
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(lr_model.optimizer, feed_dict={lr_model.X: x, lr_model.Y: y})
        #Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(lr_model.cost, feed_dict={lr_model.X: train_X, lr_model.Y:train_Y})
            print "Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(lr_model.W), "b=", sess.run(lr_model.b)
            total_epochs.append(epoch+1)
            total_loss.append(c)
            #Graphic display
            #plt.plot(train_X, train_Y, 'ro', label='Original data')
            #plt.plot(train_X, sess.run(lr_model.W) * train_X + sess.run(lr_model.b), label='Fitted line epoch: {}'.format(epoch+1))
            #plt.legend()
            #plt.show()
            
    print "Optimization Finished!"
    training_cost = sess.run(lr_model.cost, feed_dict={lr_model.X: train_X, lr_model.Y: train_Y})
    print "Training cost=", training_cost, "W=", sess.run(lr_model.W), "b=", sess.run(lr_model.b), '\n'
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(lr_model.W) * train_X + sess.run(lr_model.b), label='Fitted line epoch: {}'.format(epoch+1))
    plt.legend()
    plt.show()

"""Finalment, representació gràfica de la disminució de l'error, com podem veure, al principi hi ha molta distància entre els punts, això vol dir que es disminueix l'error en grans quantitats. Ràpidament, però, els valors comencen a ser equidistants i al mateix nivell, fent que el cost computacional de la reducció d'error sigui massa gran o simplement s'hagi arribat a un mínim (possiblement el segon)."""

plt.plot(total_epochs, total_loss, 'ro', label='Cost values (every 50 epochs)')
plt.legend()
plt.show()